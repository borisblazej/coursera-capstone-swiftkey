---
title: "Word Frequency Exploration (Week 2)"
author: "BB"
date: '2022-04-25'
output: html_document
---

```{r setup, include=FALSE}
# Libraries
library(stringr)
library(tidyverse)

# Parameters
knitr::opts_chunk$set(echo = TRUE)

# Data (see 01_Explore.Rmd and 02_Clean.R for details or run both in case of error)
clean_text <- read_rds("../data/en_US.twitter_clean.rds.gz")

head(clean_text, 10)

# Functions



```

## 1. Break text into words

```{r text splits}

words <- str_split(clean_text, "\\s") %>% unlist()
words <- str_trim(words)

# 2-grams
gram2.1 <- str_match_all(clean_text, "[\\w'\\-]+\\s[\\w'\\-]+") %>% 
    unlist()
# above only finds word pairs (1,2), (3,4), ... but not (2,3), hence:
gram2.2 <- str_replace(clean_text, "^[\\w'\\-]+\\s", "") %>% 
    str_match_all("[\\w'\\-]+\\s[\\w'\\-]+") %>% 
    unlist()
gram2 <- c(gram2.1, gram2.2)
gram2 <- str_trim(gram2)
rm(gram2.1, gram2.2)

# 3-grams
gram3.1 <- str_match_all(clean_text, "[\\w'\\-]+\\s[\\w'\\-]+\\s[\\w'\\-]+") %>% 
    unlist()
gram3.2 <- str_replace(clean_text, "^[\\w'\\-]+\\s", "") %>% 
    str_match_all("[\\w'\\-]+\\s[\\w'\\-]+\\s[\\w'\\-]+") %>% 
    unlist()
gram3.3 <- str_replace(clean_text, "^[\\w'\\-]+\\s\\s[\\w'\\-]+", "") %>% 
    str_match_all("[\\w'\\-]+\\s[\\w'\\-]+\\s[\\w'\\-]+") %>% 
    unlist()
gram3 <- c(gram3.1, gram3.2, gram3.3)
gram3 <- str_trim(gram3)

rm(gram3.1, gram3.2, gram3.3)
gc()

write_rds(words, "../data/en_US.twitter_word.rds.gz", compress = "gz")
write_rds(gram2, "../data/en_US.twitter_gram2.rds.gz", compress = "gz")
write_rds(gram3, "../data/en_US.twitter_gram3.rds.gz", compress = "gz")

head(gram3, 100)

```

## 2. Word Frequencies

Build a sorted word-frequency table, then plot the distribution from the top 1000 words and show top 20 words:

```{r word frequencies}

word_freq_raw <- as.data.frame(table(words)) 

word_freq_1 <- word_freq_raw %>%
    arrange(-Freq) %>% 
    filter(words != "")

plot(word_freq_1$Freq[1:1000])
head(word_freq_1$words, 20)

```

## 3. 2-gram/3-gram frequencies

Same procedure for 2-grams as for words:

```{r 2-gram frequencies}

gram2_freq_raw <- as.data.frame(table(gram2)) 

gram2_freq_1 <- gram2_freq_raw %>%
    arrange(-Freq)

plot(gram2_freq_1$Freq[1:1000])
gram2_freq_1$gram2[1:20]

```

... and for 3-grams:
```{r 3-gram frequencies}

gram3_freq_raw <- as.data.frame(table(gram3)) 

gram3_freq_1 <- gram3_freq_raw %>%
    arrange(-Freq)

plot(gram3_freq_1$Freq[1:1000])
gram3_freq_1$gram3[1:20]

```


# 4. Top words

How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%? 

```{r top words}

word_occurences <- sum(word_freq_1$Freq)

word_freq_2 <- word_freq_1 %>% 
    mutate(share = Freq / word_occurences,
           cum_freq = cumsum(Freq),
           cum_share = cum_freq / word_occurences)

top_50pct_words <- word_freq_2 %>% 
    filter(cum_share < 0.5) 
nrow(top_50pct_words)

top_90pct_words <- word_freq_2 %>% 
    filter(cum_share < 0.9) 
nrow(top_90pct_words)


```

